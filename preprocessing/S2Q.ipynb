{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S2Q\n",
    "\n",
    "Turning sentences to questions with dependency parsing\n",
    "\n",
    "\n",
    "1. Take the root of sentence.\n",
    "2. Look whether there's an `aux` or `cop` or `auxpass` dependent of the root, take the first of these, and move it before the subject `nsubjpass` or `nsubj`.\n",
    "3. Otherwise put \"do\" or \"does\" or \"did\" in front of the subject (by the NN tag),  check root verb's lemma, and past tense -> lemma.\n",
    "4. Remove any `RB` (adverbs) in front of the subject (`nsubj` or `nsubjpass`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser import depparse_ssplit, setup_corenlp, get_parse, Sentence\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_corenlp(\"en\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_do(subj_is_plural, past_tense):\n",
    "    if past_tense==True:\n",
    "        return \"did\"\n",
    "    elif subj_is_plural==True:\n",
    "        return \"do\"\n",
    "    else:\n",
    "        return \"does\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def why(original_sentence):\n",
    "    original_sentence = original_sentence.lower()\n",
    "    \n",
    "    # replace \"'s\" with \"is\" where appropriate\n",
    "    s = Sentence(get_parse(original_sentence), original_sentence, \"en\")\n",
    "    indices_to_replace = [t[\"index\"] for t in s.tokens if t[\"word\"] == \"'s\" and t[\"pos\"][0] == \"V\"]\n",
    "    for i in indices_to_replace:\n",
    "        s.tokens[i-1][\"word\"] = \"is\"\n",
    "        s.tokens[i-2][\"after\"] = \" \"\n",
    "    original_sentence = \"\".join(t[\"word\"] + t[\"after\"] for t in s.tokens).replace(\"  \", \" \")\n",
    "    if s.tokens[0][\"word\"].lower() == \"or\":\n",
    "        return(\"ERROR: can't transform: starts with 'or'\")\n",
    "    \n",
    "    s = Sentence(get_parse(original_sentence), original_sentence, \"en\")\n",
    "    s.new_tokens = s.tokens\n",
    "    doc = nlp(original_sentence)\n",
    "    lemmas = {token.text.lower(): token.lemma_ for token in doc}\n",
    "    past_tense = {token.text: token.tag_==\"VBD\" for token in doc}\n",
    "    \n",
    "    # 1. Take the root of sentence.\n",
    "    sentence_indices = [t[\"index\"] for t in s.tokens]\n",
    "    root_index = [i for i in sentence_indices if \"ROOT\" in s.find_dep_types(i)][0]\n",
    "    if s.token(root_index)[\"pos\"][0] == \"V\":\n",
    "        root_verb_index = root_index\n",
    "        # Find where the subject NP starts.\n",
    "        # First, find any children of the root where the dep is nsubj or nsubjpass\n",
    "        # and grap the corresponding dependent.\n",
    "        subj_head_indices = s.find_children(root_index, filter_types = [\"nsubj\", \"nsubjpass\"])\n",
    "        if len(subj_head_indices)>0:\n",
    "            subj_head_index = subj_head_indices[0]\n",
    "        else:\n",
    "            return(\"ERROR: can't transform: missing nsubj\")\n",
    "    else:\n",
    "        # assume the first verb is root (this is a hack)\n",
    "        verb_indices = [t[\"index\"] for t in s.tokens if t[\"pos\"][0] == \"V\"]\n",
    "        if len(verb_indices) > 0:\n",
    "            root_verb_index = min(verb_indices)\n",
    "        else:\n",
    "            return(\"ERROR: can't transform: weird root\")\n",
    "        \n",
    "        if s.token(root_index)[\"pos\"][0] == \"N\":\n",
    "            subj_head_index = root_index\n",
    "        else:\n",
    "            # assume the first noun is subj (this is a hack)\n",
    "            noun_indices = [t[\"index\"] for t in s.tokens if t[\"pos\"][0] == \"N\"]\n",
    "            if len(noun_indices)>0:\n",
    "                subj_head_index = min(noun_indices)\n",
    "            else:\n",
    "                return(\"ERROR: can't transform: weird root\")\n",
    "                    \n",
    "    # Then, get the index where this phrase begins.\n",
    "    subj_start_index = min(s.get_subordinate_indices([subj_head_index], [subj_head_index]))\n",
    "    subj_head = s.token(subj_head_index)[\"word\"]\n",
    "    subj_is_plural = (subj_head.lower() != lemmas[subj_head.lower()])\n",
    "    \n",
    "    if subj_head.lower() == \"who\":\n",
    "        s.tokens[subj_head_index - 1][\"word\"] = \"they\"\n",
    "        s.new_tokens = s.tokens\n",
    "\n",
    "    # 2. Look whether ther's an aux or cop or auxpass dependent of the root,\n",
    "    aux_types = [\"aux\", \"cop\", \"auxpass\"]\n",
    "    aux_indices = s.find_children(root_verb_index, filter_types = aux_types)\n",
    "    if (len(aux_indices) == 0):\n",
    "        aux_indices = [s.find_children(i, filter_types = aux_types) for i in sentence_indices]\n",
    "        aux_indices = [item for sublist in aux_indices for item in sublist]\n",
    "    has_aux = len(aux_indices) > 0\n",
    "    # If so, take the first of these, and move it before the subject `nsubjpass` or `nsubj`.\n",
    "    if has_aux:\n",
    "        first_aux_index = min(aux_indices)\n",
    "        s.move(first_aux_index, subj_start_index)\n",
    "    # 3. Otherwise put \"do\" or \"does\" or \"did\" in front of the subject,\n",
    "    # check root verb's lemma, and past tense -> lemma.\n",
    "    else:\n",
    "        # find root verb\n",
    "        # make it a lemma\n",
    "        root_verb = s.token(root_verb_index)[\"word\"]\n",
    "        if (root_verb == \"is\"):\n",
    "            s.move(root_verb_index, 1)\n",
    "        else:\n",
    "            s.token(root_verb_index)[\"word\"] = lemmas[root_verb.lower()]\n",
    "            # figre out which of \"do\" \"does\" or \"did\" to use\n",
    "            s.add({\"word\": get_do(subj_is_plural, past_tense[root_verb]),\n",
    "                   \"index\": subj_start_index, \"after\": \" \"})\n",
    "        \n",
    "    # 4. Remove any `RB` (adverbs) in front of the subject (`nsubj` or `nsubjpass`)\n",
    "    for t in s.new_tokens:\n",
    "        if \"pos\" in t.keys() and t[\"pos\"] == \"RB\":\n",
    "            s.cut(t[\"index\"])\n",
    "\n",
    "    # Add \"why\" at the beginning.\n",
    "    s.add({\"word\": \"Why\", \"index\": 1, \"after\": \" \"})\n",
    "\n",
    "    # Also, add a question mark.\n",
    "    last_word = s.new_tokens[-1][\"word\"]\n",
    "    if (last_word in [\".\"]):\n",
    "        last_word = \"?\"\n",
    "    else:\n",
    "        s.add({\"word\": \"?\", \"index\": len(s.new_tokens)+1, \"after\": \"\"})\n",
    "\n",
    "    return \"\".join([x[\"word\"] + x[\"after\"] for x in s.new_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why may the quick brown fox have jumped over the lazy dog.\n",
      "Why was the victim saved?\n",
      "Why did the quick brown fox jump over the lazy dog.\n",
      "Why are people like ice cream.\n",
      "ERROR: can't transform: weird root\n",
      "Why is the sky blue?\n",
      "Why does erin like ice cream?\n",
      "Why do little kids play in the park?\n",
      "Why is there a problem?\n",
      "ERROR: can't transform: starts with 'or'\n",
      "ERROR: can't transform: starts with 'or'\n",
      "Why '' is it frustrating .\n",
      "ERROR: can't transform: missing nsubj\n",
      "Why did they want to be identified .\n",
      "Why were they granted anonymity .\n"
     ]
    }
   ],
   "source": [
    "print(why(\"The quick brown fox may have jumped over the lazy dog.\"))\n",
    "print(why(\"The victim was only saved\"))\n",
    "print(why(\"The quick brown fox jumped over the lazy dog.\"))\n",
    "print(why(\"People are like ice cream.\"))\n",
    "print(why(\"People like ice cream.\"))\n",
    "print(why(\"The sky is blue\"))\n",
    "print(why(\"Erin likes ice cream\"))\n",
    "print(why(\"Little kids play in the park\"))\n",
    "print(why(\"There's a problem\"))\n",
    "print(why(\"Or perhaps it's\"))\n",
    "print(why(\"Or perhaps it 's\"))\n",
    "print(why(\"'' It 's frustrating .\"))\n",
    "print(why(\"To talk about this .\"))\n",
    "print(why(\"Who did not want to be identified .\"))\n",
    "print(why(\"Who were granted anonymity .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Sentence(get_parse(\"There is a problem\"),\n",
    "         original_sentence = \"There is a problem\", lang=\"en\")\n",
    "# [t[\"index\"] for t in s.tokens if t[\"pos\"][0] == \"V\"]\n",
    "# s.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
