#! /usr/bin/env python
# -*- coding: utf-8 -*-

import os
import io
import json
import argparse
import logging
from itertools import izip

import random
import numpy as np
from util import rephrase
from os.path import join as pjoin
from os.path import dirname, abspath

from cfg import DISCOURSE_MARKER_SET_TAG, EN_DISCOURSE_MARKERS

import sys

reload(sys)
sys.setdefaultencoding('utf8')

np.random.seed(123)
random.seed(123)

"""
Add additional postprocessing steps for (s1, s2) here, such as delete punctuations

We apply filtering to balance s1 and s2 length
Merge them into one set, train/val/test split, np.shuffle (fix random seed)

(then Torchtext can take it from there!)
"""

parser = argparse.ArgumentParser(description='DisExtract Producer')

# parser.add_argument("--json", type=str, default="example_config.json", help="load in config params")
parser.add_argument("--corpus", type=str, default='books',
                    help="books|gigaword, marked by Spanish and Chinese")
parser.add_argument("--train_size", default=0.9, type=float)
parser.add_argument("--max_seq_len", default=50, type=int)
parser.add_argument("--min_seq_len", default=5, type=int)
parser.add_argument("--max_ratio", default=5.0, type=float)
parser.add_argument("--data_dir", type=str, default='default', help="the path for the data file")
parser.add_argument("--data_file", type=str, required=True,
                    help="Load in the tsv file name generated by bookcorpus.py or gigaword.py")
parser.add_argument("--out_prefix", type=str, required=True,
                    help="Prefix the produced files")
parser.add_argument("--balanced", action='store_true', help="use this flag to cut all markers off at the minimum count")
parser.add_argument("--count_per_marker", type=int, default=-1, help="use this for modifying the cutoff for a 'balanced' dataset, by default perfectly balanced")
parser.add_argument("--exclude", type=str, default="")

args, _ = parser.parse_known_args()
args.min_ratio = 1 / args.max_ratio  # auto-generate min-ratio

# ======== Split =========

assert (args.train_size < 1 and args.train_size > 0)
split_proportions = {
    "train": args.train_size,
    "valid": (1 - args.train_size) / 2,
    "test": (1 - args.train_size) / 2
}
assert (sum([split_proportions[split] for split in split_proportions]) == 1)

print("the data split is: {}".format(split_proportions))

# ======== Data Path =========
if args.data_dir == "default":
    root_dir = dirname(dirname(abspath(__file__)))
    args.data_dir = pjoin(root_dir, "data", args.corpus)


def write_to_tsv(data, file_name):
    with open(file_name, 'wb') as f:
        for line in data:
            f.write(line)

def add_one_to_dict(dic, entry):
    if entry in dic:
        dic[entry] += 1
    else:
        dic[entry] = 1

if __name__ == '__main__':

    examples = []

    with open(pjoin(args.data_dir, args.data_file), 'rb') as f:
        for line in f:
            examples.append(line)

    # ==== Filtering =====
    data_dist = {}
    filtered_examples = {}
    number_of_filtered_examples = 0
    for ex in examples:
        s1, s2, label = ex[:-1].split('\t')
        ratio = float(len(s1)) / max(len(s2), 0.0001)
        if len(s1) < args.min_seq_len or args.max_seq_len < len(s1):
            continue
        elif len(s2) < args.min_seq_len or args.max_seq_len < len(s2):
            continue
        elif ratio < args.min_ratio or args.max_ratio < ratio:
            continue
        else:
            example_line = "\t".join([s1, s2, label]) + "\n"
            if label in filtered_examples:
                filtered_examples[label].append(example_line)
            else:
                filtered_examples[label] = [example_line]
            # filtered_examples.append("\t".join([s1, s2, label]))
            # collect stats
            add_one_to_dict(data_dist, label)
            number_of_filtered_examples+=1

    print("original number: {}, filtered out number: {}".format(len(examples), number_of_filtered_examples))

    print("label distribution: {}".format(data_dist))

    minimum_count_per_marker = min(data_dist.values())

    exclude_marker_list = args.exclude.split(",")

    examples = []
    for label in filtered_examples:
        if label in exclude_marker_list:
            pass
        else:
            if args.balanced:
                random.shuffle(filtered_examples[label])
                if args.count_per_marker == -1:
                    count_per_marker = minimum_count_per_marker
                else:
                    count_per_marker = args.count_per_marker
                examples += filtered_examples[label][:count_per_marker]
            else:
                examples += filtered_examples[label]

    print "total number in produced dataset: {}".format(len(examples))

    serial_numbers = range(len(examples))
    random.shuffle(serial_numbers)

    train_numbers = serial_numbers[:int(np.rint(len(examples) * split_proportions['train']))]
    valid_numbers = serial_numbers[
                    int(np.rint(len(examples) * split_proportions['train'])): \
                        int(np.rint(len(examples) * (split_proportions['train'] + split_proportions['valid'])))]
    test_numbers = serial_numbers[
                   int(np.rint(len(examples) * (split_proportions['train'] + split_proportions['valid']))):]

    print(
    "train/valid/test number of examples: {}/{}/{}".format(len(train_numbers), len(valid_numbers), len(test_numbers)))

    train, valid, test = [], [], []

    for tn in train_numbers:
        train.append(examples[tn])
    for tn in valid_numbers:
        valid.append(examples[tn])
    for tn in test_numbers:
        test.append(examples[tn])

    # Note that under default setting, corpus is already appended
    write_to_tsv(train, pjoin(args.data_dir, args.out_prefix + "_train.tsv"))
    write_to_tsv(valid, pjoin(args.data_dir, args.out_prefix + "_valid.tsv"))
    write_to_tsv(test, pjoin(args.data_dir, args.out_prefix +  "_test.tsv"))
